{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-73187c3fb20f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TensorFlow and tf.keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m  \u001b[1;31m# It is important in neural networks to scale the date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m  \u001b[1;31m# The standard - train/test to prevent overfitting and choose hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "After we load the data, we print the shape of the data and a pixelated digit.\n",
    "\n",
    "We also show what the features of one example looks like.\n",
    "\n",
    "The neural net will learn to estimate which digit these pixels represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      symbol    date_obs  open_pr  high_pr   low_pr  close_pr  close_adj  \\\n",
      "0      MSFT  2016-11-16   58.940   59.660   58.810     59.65    58.3529   \n",
      "1      MSFT  2016-11-17   60.410   60.950   59.965     60.64    59.3213   \n",
      "2      MSFT  2016-11-18   60.780   61.140   60.300     60.35    59.0376   \n",
      "3      MSFT  2016-11-21   60.500   60.970   60.420     60.86    59.5365   \n",
      "4      MSFT  2016-11-22   60.980   61.260   60.805     61.12    59.7909   \n",
      "...     ...         ...      ...      ...      ...       ...        ...   \n",
      "1144   MSFT  2021-06-07  249.980  254.090  249.810    253.81   253.8100   \n",
      "1145   MSFT  2021-06-08  255.160  256.010  252.510    252.57   252.5700   \n",
      "1146   MSFT  2021-06-09  253.810  255.530  253.210    253.59   253.5900   \n",
      "1147   MSFT  2021-06-10  254.290  257.460  253.670    257.24   257.2400   \n",
      "1148   MSFT  2021-06-11  257.985  258.487  256.610    257.89   257.8900   \n",
      "\n",
      "        volume  split_coeff  \n",
      "0     27332475            1  \n",
      "1     32132728            1  \n",
      "2     27686311            1  \n",
      "3     19652595            1  \n",
      "4     23206700            1  \n",
      "...        ...          ...  \n",
      "1144  23079167            1  \n",
      "1145  22454998            1  \n",
      "1146  17937634            1  \n",
      "1147  24563619            1  \n",
      "1148  18999731            1  \n",
      "\n",
      "[1149 rows x 9 columns]>\n",
      "[0.000e+00 1.000e+00 2.000e+00 ... 1.666e+03 1.667e+03 1.668e+03]\n",
      "[[58.94 59.66 58.81 ... 27332475 1 0.0]\n",
      " [60.41 60.95 59.965 ... 32132728 1 1.0]\n",
      " [60.78 61.14 60.3 ... 27686311 1 2.0]\n",
      " ...\n",
      " [253.81 255.53 253.21 ... 17937634 1 1666.0]\n",
      " [254.29 257.46 253.67 ... 24563619 1 1667.0]\n",
      " [257.985 258.487 256.61 ... 18999731 1 1668.0]]\n",
      "(1149, 7)\n",
      "['2016-11-16' '2016-11-17' '2016-11-18' ... '2021-06-09' '2021-06-10'\n",
      " '2021-06-11']\n",
      "[58.3529 59.3213 59.0376 ... 253.59 257.24 257.89]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Turn this into a fuction so we can do many stocks quickly\n",
    "\n",
    "msft= pd.read_csv('msft.csv')\n",
    "print(msft.head)\n",
    "\n",
    "# Create New Feature 'Days Since Start' for temporal data\n",
    "start_date = datetime.strptime(msft.values[0,1], \"%Y-%m-%d\")\n",
    "days_since = np.zeros(msft.values.shape[0])\n",
    "\n",
    "for i in range(len(days_since)):\n",
    "    curr_date = datetime.strptime(msft.values[i,1], \"%Y-%m-%d\")\n",
    "    diff = curr_date - start_date\n",
    "    \n",
    "    \n",
    "    days_since[i] = diff.days\n",
    "\n",
    "print(days_since)\n",
    "\n",
    "X = np.hstack((np.hstack((msft.values[:,2:6],msft.values[:,7:])), days_since.reshape(-1,1)))\n",
    "msft_date = msft.values[:,1]\n",
    "\n",
    "# Target is close_adj\n",
    "y = msft.values[:,6]\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(msft_date)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Train Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[209.56 211.33 205.03 ... 32248893 1 1337.0]\n",
      " [205.4 205.7 202.31 ... 29940718 1 1338.0]\n",
      " [204.47 205.04 201.39 ... 31636070 1 1339.0]\n",
      " ...\n",
      " [253.81 255.53 253.21 ... 17937634 1 1666.0]\n",
      " [254.29 257.46 253.67 ... 24563619 1 1667.0]\n",
      " [257.985 258.487 256.61 ... 18999731 1 1668.0]]\n"
     ]
    }
   ],
   "source": [
    "#Split the data into training and test set.  Train on first 80%, test and see results on last 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.39858502 -1.38655508 -1.39304743 ... -0.06812608  0.\n",
      "  -1.73443655]\n",
      " [-1.35882573 -1.35217714 -1.36132082 ...  0.28284549  0.\n",
      "  -1.73184153]\n",
      " [-1.34881829 -1.34711372 -1.35211873 ... -0.04225528  0.\n",
      "  -1.72924652]\n",
      " ...\n",
      " [ 2.78506602  2.72867049  2.78964652 ... -0.18063395  0.\n",
      "   1.72212505]\n",
      " [ 2.80832655  2.77450774  2.66383882 ...  0.72176261  0.\n",
      "   1.72991009]\n",
      " [ 2.58248297  2.58929328  2.5410527  ...  0.68244892  0.\n",
      "   1.73250511]]\n",
      "[[ 2.67525464  2.65538419  2.62345949 ...  0.29133892  0.\n",
      "   1.73510013]\n",
      " [ 2.56273856  2.50534715  2.548744   ...  0.12257619  0.\n",
      "   1.73769514]\n",
      " [ 2.53758472  2.48775844  2.52347258 ...  0.24653223  0.\n",
      "   1.74029016]\n",
      " ...\n",
      " [ 3.87209042  3.83329489  3.94691253 ... -0.75503197  0.\n",
      "   2.58886046]\n",
      " [ 3.88507305  3.88472855  3.95954824 ... -0.2705716   0.\n",
      "   2.59145548]\n",
      " [ 3.98501222  3.91209765  4.04030689 ... -0.67737651  0.\n",
      "   2.59405049]]\n"
     ]
    }
   ],
   "source": [
    "# I read somewhere that it is important to scale the test data off of the training data. Something about assuming same parameters idk\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled) # Looking the new features after scaling\n",
    "print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208.04 203.92 202.88 211.6 208.75 211.75 202.54 201.3 203.85 202.02\n",
      " 204.06 203.9 205.01 216.54 213.29 212.94 216.35 212.48 208.25 203.38\n",
      " 209.19 208.7 208.9 209.77 210.977 209.7 214.58 213.02 213.69 216.47\n",
      " 221.15 226.58 228.91 225.53 227.27 231.65 217.3 214.25 202.66 211.29\n",
      " 205.37 204.03 205.41 208.78 205.05 202.91 200.39 202.54 207.42 200.59\n",
      " 203.19 207.82 209.44 207.26 210.33 212.46 206.19 210.38 205.91 209.83\n",
      " 210.58 215.81 221.4 222.86 220.86 219.66 219.66 214.22 214.65 214.8\n",
      " 214.89 216.23 210.08 213.25 202.68 204.72 202.47 202.33 206.43 216.39\n",
      " 223.29 223.72 218.39 211.01 216.55 215.44 216.51 216.655 213.893 211.08\n",
      " 212.42 210.39 210.11 213.86 213.87 215.23 214.07 216.21 215.37 214.24\n",
      " 214.36 214.29 216.01 211.8 210.52 213.26 214.2 214.13 219.28 219.42\n",
      " 218.59 222.59 223.94 221.02 222.75 224.96 224.15 221.68 222.42 217.69\n",
      " 217.9 212.25 218.29 219.62 217.49 214.93 216.34 213.02 212.65 216.44\n",
      " 224.34 224.97 225.95 229.53 232.33 232.9 238.93 231.96 239.65 239.51\n",
      " 243.0 242.01 242.2 242.47 243.77 242.82 244.49 244.99 243.142 244.2\n",
      " 243.79 240.97 234.51 233.27 234.55 228.99 232.38 236.94 233.87 227.56\n",
      " 226.73 231.6 227.39 233.78 232.42 237.13 235.75 234.81 237.71 237.04\n",
      " 230.72 230.35 235.99 237.58 235.46 232.34 236.48 235.24 231.85 235.77\n",
      " 242.35 249.07 247.86 249.9 253.25 255.85 255.91 258.49 255.59 259.5\n",
      " 260.74 258.74 258.26 260.58 257.17 261.15 261.55 261.97 254.56 252.51\n",
      " 252.18 251.86 247.79 246.47 249.73 252.46 247.18 246.23 239.0 243.03\n",
      " 248.15 245.18 242.521 243.12 246.48 245.17 250.78 251.72 251.49 249.31\n",
      " 249.68 247.4 247.3 245.71 250.79 253.81 252.57 253.59 257.24 257.89]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Creating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The activation function and its derivative\n",
    "\n",
    "We will use the sigmoid activation function:  $f(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "The deriviative of the sigmoid function is: $f'(z) = f(z)(1-f(z))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    \"\"\" # ReLU\n",
    "    new = z\n",
    "    for i in range(len(new)):\n",
    "        new[i] = max(0,new[i])\n",
    "    return new\"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-z.astype(float)))\n",
    "\n",
    "\n",
    "def f_deriv(z):\n",
    "    \"\"\"# ReLU\n",
    "    new = z\n",
    "    for i in range(len(new)):\n",
    "        if new[i] < 0:\n",
    "            new[i] = 0\n",
    "        else:\n",
    "            new[i] = 1\n",
    "    return new\"\"\"\n",
    "\n",
    "    return f(z) * (1 - f(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initialing W and b\n",
    "We want the weights in W to be different so that during back propagation the nodes on a level will have different gradients and thus have different update values.\n",
    "\n",
    "We want the  weights to be small values, since the sigmoid is almost \"flat\" for large inputs.\n",
    "\n",
    "Next is the code that assigns each weight a number uniformly drawn from $[0.0, 1.0)$.  The code assumes that the number of neurons in each level is in the python list *nn_structure*.\n",
    "\n",
    "In the code, the weights, $W^{(\\ell)}$ and $b^{(\\ell)}$ are held in a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing $\\triangledown W$ and $\\triangledown b$\n",
    "Creating $\\triangledown W^{(\\ell)}$ and $\\triangledown b^{(\\ell)}$ to have the same size as $W^{(\\ell)}$ and $b^{(\\ell)}$, and setting $\\triangledown W^{(\\ell)}$, and  $\\triangledown b^{(\\ell)}$ to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]),dtype='float64')\n",
    "        tri_b[l] = np.zeros((nn_structure[l],),dtype='float64')\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "Perform a forward pass throught the network.  The function returns the values of $a$ and $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        if l == len(W):\n",
    "            node_in = a[l]\n",
    "            z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "            a[l+1] = z[l+1]\n",
    "        else:\n",
    "            node_in = a[l]\n",
    "            z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "            a[l+1] = f(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\delta$\n",
    "The code below compute $\\delta^{(s_l)}$ in a function called \"calculate_out_layer_delta\",  and  computes $\\delta^{(\\ell)}$ for the hidden layers in the function called \"calculate_hidden_delta\".  \n",
    "\n",
    "If we wanted to have a different cost function, we would change the \"calculate_out_layer_delta\" function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out)\n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Back Propagation Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i]-a[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = a[n_layers]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural network\n",
    "\n",
    "Our code assumes the size of each layer in our network is held in a list.  The input layer will have 64 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons (you can change this value).  The output layer has 10 neurons.\n",
    "\n",
    "Next we create the python list to hold the number of neurons for each level and then run the neural network code with our training data.\n",
    "\n",
    "This code will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 1500 iterations\n",
      "Iteration 0 of 1500\n",
      "Iteration 1000 of 1500\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [7, 4, 2, 1]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train_scaled, y_train, 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the avg_cost_func\n",
    "plt.plot(avg_cost_func)\n",
    "plt.ylabel('Average J')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Assessing accuracy\n",
    "Next we determine what percentage the neural network correctly predicted the handwritten digit correctly on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-178-41fd3dfa12c7>:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z.astype(float)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-bca3d47365a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get the prediction accuracy and print\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-183-29441b534dec>\u001b[0m in \u001b[0;36mpredict_y\u001b[1;34m(W, b, X, n_layers)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# get the prediction accuracy and print\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
